<p align="center">

  <h2 align="center">Dyadic Interaction Modeling for Social Behavior Generation</h2>
  <p align="center">
    <a href="https://scholar.google.com/citations?hl=en&user=HuuQRj4AAAAJ"><strong>Minh Tran</strong></a><sup>*</sup>
    ·  
    <a href="https://boese0601.github.io/"><strong>Di Chang</strong></a><sup>*</sup>
    ·
    <a href="https://scholar.google.com/citations?user=5w0f0OQAAAAJ&hl=ru"><strong>Maksim Siniukov</strong></a>
    ·
    <a href="https://www.ihp-lab.org/"><strong>Mohammad Soleymani</strong></a>
    <br>
    University of Southern California
    <br>
    <sup>*</sup>Equal Contribution
    <br>
    </br>
        <a href="https://arxiv.org/abs/2403.09069">
        <img src='https://img.shields.io/badge/arXiv-DIM-green' alt='Paper PDF'>
        </a>
        <a href='https://boese0601.github.io/dim/'>
        <img src='https://img.shields.io/badge/Project_Page-DIM-blue' alt='Project Page'></a>
        <!-- <a href='https://youtu.be/VPJe6TyrT-Y'>
        <img src='https://img.shields.io/badge/YouTube-MagicPose-rgb(255, 0, 0)' alt='Youtube'></a> -->
     </br>
    <table align="center">
        <img src="./assets/demo1.gif">
        <img src="./assets/demo2.gif">
    </table>
</p>

*We propose Dyadic Interaction Modeling, a pre-training strategy that jointly models speakers’ and listeners’ motions and learns representations that capture the dyadic context. We then utilize the pre-trained weights and feed multimodal inputs from the speaker into DIM-Listener. DIM-Listener is capable of generating photorealistic videos for the listener's motion.*



## News
* **[2024.6.23]** Code will be released soon.
* **[2024.6.23]** Release Dyadic Interaction Modeling project page.
* **[2024.3.27]** Release Dyadic Interaction Modeling paper.




## Citing
If you find our work useful, please consider citing:
```BibTeX
@article{tran2024dyadic,
      title={Dyadic Interaction Modeling for Social Behavior Generation},
      author={Tran, Minh and Chang, Di and Siniukov, Maksim and Soleymani, Mohammad},
      journal={arXiv preprint arXiv:2403.09069},
      year={2024}
}
```

## Acknowledgement

We appreciate the support from [Haiwen Feng](https://scholar.google.com/citations?user=g5co-iIAAAAJ&hl=en), [Quankai Gao](https://zerg-overmind.github.io/) and [Hongyi Xu](https://hongyixu37.github.io/homepage/) for their suggestions and discussions.


