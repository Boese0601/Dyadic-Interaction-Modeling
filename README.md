<p align="center">

  <h2 align="center">Dyadic Interaction Modeling for Social Behavior Generation</h2>
  <p align="center">
    <a href="https://scholar.google.com/citations?hl=en&user=HuuQRj4AAAAJ"><strong>Minh Tran</strong></a><sup>*</sup>
    ·  
    <a href="https://boese0601.github.io/"><strong>Di Chang</strong></a><sup>*</sup>
    ·
    <a href="https://scholar.google.com/citations?user=5w0f0OQAAAAJ&hl=ru"><strong>Maksim Siniukov</strong></a>
    ·
    <a href="https://www.ihp-lab.org/"><strong>Mohammad Soleymani</strong></a>
    <br>
    University of Southern California
    <br>
    <sup>*</sup>Equal Contribution
    <br>
    </br>
        <a href="https://arxiv.org/abs/2403.09069">
        <img src='https://img.shields.io/badge/arXiv-DIM-green' alt='Paper PDF'>
        </a>
        <a href='https://boese0601.github.io/dim/'>
        <img src='https://img.shields.io/badge/Project_Page-DIM-blue' alt='Project Page'></a>
        <!-- <a href='https://youtu.be/VPJe6TyrT-Y'>
        <img src='https://img.shields.io/badge/YouTube-MagicPose-rgb(255, 0, 0)' alt='Youtube'></a> -->
     </br>
    <table align="center">
        <img src="./assets/demo1.gif">
        <img src="./assets/demo2.gif">
    </table>
</p>

*We propose Dyadic Interaction Modeling, a pre-training strategy that jointly models speakers’ and listeners’ motions and learns representations that capture the dyadic context. We then utilize the pre-trained weights and feed multimodal inputs from the speaker into DIM-Listener. DIM-Listener is capable of generating photorealistic videos for the listener's motion.*



## News
* **[2024.6.23]** Code is fully released. Instructions on training and inference coming soon.
* **[2024.6.23]** Release Dyadic Interaction Modeling project page.
* **[2024.3.27]** Release Dyadic Interaction Modeling paper.




## Citing
If you find our work useful, please consider citing:
```BibTeX
@article{tran2024dyadic,
      title={Dyadic Interaction Modeling for Social Behavior Generation},
      author={Tran, Minh and Chang, Di and Siniukov, Maksim and Soleymani, Mohammad},
      journal={arXiv preprint arXiv:2403.09069},
      year={2024}
}
```

## License

Our code is distributed under the USC research license. See `LICENSE.txt` file for more information.

## Acknowledgement
This work is supported by the National Science Foundation under Grant No. 2211550. The work was also sponsored by the Army Research Office and was accomplished under Cooperative Agreement Number W911NF-20-2-0053. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.

We appreciate the support from [Haiwen Feng](https://scholar.google.com/citations?user=g5co-iIAAAAJ&hl=en), [Quankai Gao](https://zerg-overmind.github.io/) and [Hongyi Xu](https://hongyixu37.github.io/homepage/) for their suggestions and discussions.


